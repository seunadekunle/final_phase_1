{
    "version_id": "v1.1.0_20241129_181449_7d57606a",
    "semantic_version": "1.1.0",
    "timestamp": "20241129_181449",
    "config_hash": "7d57606a",
    "model_config": {
        "name": "StyleClassifierV2",
        "version": "1.1.0",
        "description": "Improved version with residual connections and batch normalization",
        "architecture": {
            "base_model": "CLIP ViT-B/32",
            "hidden_size": 512,
            "dropout_rate": 0.2,
            "use_residual": true,
            "num_categories": 49,
            "num_attributes": 26
        },
        "training": {
            "batch_size": 32,
            "learning_rate": 0.0002,
            "num_epochs": 50,
            "optimizer": "Adam",
            "weight_decay": 0.0001,
            "scheduler": "ReduceLROnPlateau"
        }
    },
    "total_epochs": 5,
    "best_val_loss": 1.2307240000137916,
    "architecture_summary": "StyleClassifierV2(\n  (clip): CLIPModel(\n    (text_model): CLIPTextTransformer(\n      (embeddings): CLIPTextEmbeddings(\n        (token_embedding): Embedding(49408, 512)\n        (position_embedding): Embedding(77, 512)\n      )\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            )\n            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (vision_model): CLIPVisionTransformer(\n      (embeddings): CLIPVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n        (position_embedding): Embedding(50, 768)\n      )\n      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n  )\n  (category_classifier): Sequential(\n    (0): ResidualBlock(\n      (layers): Sequential(\n        (0): Linear(in_features=768, out_features=512, bias=True)\n        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Dropout(p=0.2, inplace=False)\n        (4): Linear(in_features=512, out_features=768, bias=True)\n        (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): ResidualBlock(\n      (layers): Sequential(\n        (0): Linear(in_features=768, out_features=512, bias=True)\n        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Dropout(p=0.2, inplace=False)\n        (4): Linear(in_features=512, out_features=768, bias=True)\n        (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): ReLU()\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=768, out_features=49, bias=True)\n  )\n  (attribute_classifier): Sequential(\n    (0): ResidualBlock(\n      (layers): Sequential(\n        (0): Linear(in_features=768, out_features=512, bias=True)\n        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Dropout(p=0.2, inplace=False)\n        (4): Linear(in_features=512, out_features=768, bias=True)\n        (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): ResidualBlock(\n      (layers): Sequential(\n        (0): Linear(in_features=768, out_features=512, bias=True)\n        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU()\n        (3): Dropout(p=0.2, inplace=False)\n        (4): Linear(in_features=512, out_features=768, bias=True)\n        (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (4): ReLU()\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=768, out_features=26, bias=True)\n  )\n)",
    "model_parameters": 154496076,
    "trainable_parameters": 3218763
}