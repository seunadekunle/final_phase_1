{
    "version_id": "v1.1.0_20241130_043625_f9377e7c",
    "semantic_version": "1.1.0",
    "timestamp": "20241130_043625",
    "config_hash": "f9377e7c",
    "model_config": {
        "data": {
            "data_dir": "data_deepfashion",
            "batch_size": 32,
            "num_workers": 4,
            "image_size": 224,
            "category_list_file": "Anno_fine/list_category_cloth.txt",
            "attribute_list_file": "Anno_fine/list_attr_cloth.txt",
            "train_split": "Anno_fine/train.txt",
            "val_split": "Anno_fine/val.txt",
            "test_split": "Anno_fine/test.txt",
            "train_category": "Anno_fine/train_cate.txt",
            "val_category": "Anno_fine/val_cate.txt",
            "test_category": "Anno_fine/test_cate.txt",
            "train_attr": "Anno_fine/train_attr.txt",
            "val_attr": "Anno_fine/val_attr.txt",
            "test_attr": "Anno_fine/test_attr.txt",
            "train_bbox": "Anno_fine/train_bbox.txt",
            "val_bbox": "Anno_fine/val_bbox.txt",
            "test_bbox": "Anno_fine/test_bbox.txt"
        },
        "model": {
            "clip_model": "openai/clip-vit-base-patch32",
            "hidden_dim": 512,
            "dropout": 0.1,
            "label_smoothing": 0.1,
            "mixup_alpha": 0.2,
            "num_attention_heads": 8,
            "num_shared_layers": 2
        },
        "training": {
            "epochs": 30,
            "learning_rate": 0.0001,
            "weight_decay": 0.01,
            "warmup_epochs": 2,
            "gradient_clip_val": 1.0,
            "log_every_n_steps": 10
        },
        "seed": 42,
        "device": "mps"
    },
    "total_epochs": 8,
    "best_val_loss": 2.938116376216595,
    "architecture_summary": "StyleClassifierV2(\n  (clip): CLIPModel(\n    (text_model): CLIPTextTransformer(\n      (embeddings): CLIPTextEmbeddings(\n        (token_embedding): Embedding(49408, 512)\n        (position_embedding): Embedding(77, 512)\n      )\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            )\n            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (vision_model): CLIPVisionTransformer(\n      (embeddings): CLIPVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n        (position_embedding): Embedding(50, 768)\n      )\n      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n  )\n  (attention): AttentionBlock(\n    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (attn): MultiheadAttention(\n      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (shared_layers): ModuleList(\n    (0-1): 2 x ResidualBlock(\n      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (ff): Sequential(\n        (0): Linear(in_features=768, out_features=512, bias=True)\n        (1): GELU(approximate='none')\n        (2): Dropout(p=0.1, inplace=False)\n        (3): Linear(in_features=512, out_features=768, bias=True)\n        (4): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (category_classifier): Sequential(\n    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): GELU(approximate='none')\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Linear(in_features=512, out_features=50, bias=True)\n  )\n  (attribute_classifier): Sequential(\n    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (1): Linear(in_features=768, out_features=512, bias=True)\n    (2): GELU(approximate='none')\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Linear(in_features=512, out_features=26, bias=True)\n  )\n)",
    "model_parameters": 156052301,
    "trainable_parameters": 4774988
}