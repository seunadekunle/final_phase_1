{
    "version": "v1.0.0-fcb0cc91",
    "timestamp": "2024-11-29T16:18:53.521186",
    "architecture": "StyleClassifier(\n  (clip): CLIPModel(\n    (text_model): CLIPTextTransformer(\n      (embeddings): CLIPTextEmbeddings(\n        (token_embedding): Embedding(49408, 512)\n        (position_embedding): Embedding(77, 512)\n      )\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            )\n            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (vision_model): CLIPVisionTransformer(\n      (embeddings): CLIPVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n        (position_embedding): Embedding(50, 768)\n      )\n      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n  )\n  (category_classifier): Sequential(\n    (0): Linear(in_features=768, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=512, out_features=49, bias=True)\n  )\n  (attribute_classifier): Sequential(\n    (0): Linear(in_features=768, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=512, out_features=26, bias=True)\n  )\n)",
    "architecture_hash": "fcb0cc91",
    "parameters": 152103244,
    "trainable_parameters": 825931,
    "layers": [
        {
            "name": "clip",
            "parameters": 151277313,
            "type": "CLIPModel"
        },
        {
            "name": "category_classifier",
            "parameters": 418865,
            "type": "Sequential"
        },
        {
            "name": "attribute_classifier",
            "parameters": 407066,
            "type": "Sequential"
        }
    ]
}