{
    "version_id": "v1.0.0_20241129_164457_9e810e16",
    "semantic_version": "1.0.0",
    "timestamp": "20241129_164457",
    "config_hash": "9e810e16",
    "model_config": {
        "name": "StyleClassifier",
        "version": "1.0.0",
        "description": "Initial version of StyleClassifier using CLIP embeddings",
        "architecture": {
            "base_model": "CLIP ViT-B/32",
            "hidden_size": 512,
            "dropout_rate": 0.1,
            "num_categories": 49,
            "num_attributes": 26
        },
        "training": {
            "batch_size": 32,
            "learning_rate": 0.0001,
            "num_epochs": 30,
            "optimizer": "Adam"
        }
    },
    "total_epochs": 21,
    "best_val_loss": 1.1653926177666738,
    "architecture_summary": "StyleClassifier(\n  (clip): CLIPModel(\n    (text_model): CLIPTextTransformer(\n      (embeddings): CLIPTextEmbeddings(\n        (token_embedding): Embedding(49408, 512)\n        (position_embedding): Embedding(77, 512)\n      )\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            )\n            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (vision_model): CLIPVisionTransformer(\n      (embeddings): CLIPVisionEmbeddings(\n        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n        (position_embedding): Embedding(50, 768)\n      )\n      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (encoder): CLIPEncoder(\n        (layers): ModuleList(\n          (0-11): 12 x CLIPEncoderLayer(\n            (self_attn): CLIPSdpaAttention(\n              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): CLIPMLP(\n              (activation_fn): QuickGELUActivation()\n              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n  )\n  (category_classifier): Sequential(\n    (0): Linear(in_features=768, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=512, out_features=49, bias=True)\n  )\n  (attribute_classifier): Sequential(\n    (0): Linear(in_features=768, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.1, inplace=False)\n    (3): Linear(in_features=512, out_features=26, bias=True)\n  )\n)",
    "model_parameters": 152103244,
    "trainable_parameters": 825931
}